{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63578e1f-72af-4372-9af0-93575770e51c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn_crfsuite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CRF\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, precision_recall_curve, auc\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "# Set seaborn style for better visuals\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "# Step 1: Load CoNLL-2003 dataset\n",
    "conll2003 = load_dataset(\"conll2003\")\n",
    "label_names = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "# Step 2: Prepare data for CRF\n",
    "def prepare_data_for_crf(dataset):\n",
    "    sentences, labels = [], []\n",
    "    for example in dataset:\n",
    "        words = example['tokens']\n",
    "        ner_tags = [label_names[tag] for tag in example['ner_tags']]\n",
    "        sentences.append(words)\n",
    "        labels.append(ner_tags)\n",
    "    return sentences, labels\n",
    "\n",
    "X_train, y_train = prepare_data_for_crf(conll2003['train'])\n",
    "X_val, y_val = prepare_data_for_crf(conll2003['validation'])\n",
    "X_test, y_test = prepare_data_for_crf(conll2003['test'])\n",
    "\n",
    "# Step 3: Enhanced NER Tag Distribution Plot\n",
    "def plot_ner_distribution(y_data, title):\n",
    "    label_counts = Counter(tag for seq in y_data for tag in seq)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=list(label_counts.keys()), \n",
    "        y=list(label_counts.values()), \n",
    "        hue=list(label_counts.keys()), \n",
    "        palette=\"viridis\", \n",
    "        legend=False\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(title, fontsize=14, pad=10)\n",
    "    plt.xlabel(\"NER Tags\", fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_ner_distribution(y_train, \"NER Tag Distribution in Training Set\")\n",
    "plot_ner_distribution(y_val, \"NER Tag Distribution in Validation Set\")\n",
    "plot_ner_distribution(y_test, \"NER Tag Distribution in Test Set\")\n",
    "\n",
    "# Step 4: Feature extraction for CRF\n",
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper': word.isupper(),\n",
    "        'word.istitle': word.istitle(),\n",
    "        'word.isdigit': word.isdigit()\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            'prev_word.lower': word1.lower(),\n",
    "            'prev_word.istitle': word1.istitle(),\n",
    "            'prev_word.isupper': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            'next_word.lower': word1.lower(),\n",
    "            'next_word.istitle': word1.istitle(),\n",
    "            'next_word.isupper': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def extract_features(sentences):\n",
    "    return [[word2features(sent, i) for i in range(len(sent))] for sent in sentences]\n",
    "\n",
    "X_train_feats = extract_features(X_train)\n",
    "X_val_feats = extract_features(X_val)\n",
    "X_test_feats = extract_features(X_test)\n",
    "\n",
    "# Step 5: Train CRF Model\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train_feats, y_train)\n",
    "\n",
    "# Step 6: Evaluation with all plots\n",
    "def evaluate_model(X, y_true, split_name, labels):\n",
    "    y_pred = crf.predict(X)\n",
    "    y_true_flat = [item for sublist in y_true for item in sublist]\n",
    "    y_pred_flat = [item for sublist in y_pred for item in sublist]\n",
    "    \n",
    "    print(f\"{split_name} Classification Report:\")\n",
    "    report = classification_report(y_true_flat, y_pred_flat, target_names=labels, output_dict=True)\n",
    "    print(classification_report(y_true_flat, y_pred_flat, target_names=labels))\n",
    "    \n",
    "    # Precision-Recall Curve (using marginal probabilities)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plotted_curves = False\n",
    "    for label in ['B-PER', 'I-PER', 'B-LOC', 'O']:\n",
    "        try:\n",
    "            label_idx = labels.index(label)\n",
    "            y_true_binary = [1 if tag == label else 0 for tag in y_true_flat]\n",
    "            # Flatten marginal probabilities across all sequences\n",
    "            marginals = [prob[label] for seq in crf.predict_marginals(X) for prob in seq]\n",
    "            y_score = marginals[:len(y_true_binary)]  # Ensure length matches\n",
    "            precision, recall, _ = precision_recall_curve(y_true_binary, y_score)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            plt.plot(recall, precision, label=f'{label} (AUC = {pr_auc:.2f})')\n",
    "            plotted_curves = True\n",
    "        except (ValueError, KeyError):\n",
    "            continue\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title(f'Precision-Recall Curve - {split_name}', fontsize=14)\n",
    "    if plotted_curves:\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No valid curves plotted', ha='center', va='center', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_flat, y_pred_flat, labels=labels)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        xticklabels=labels, \n",
    "        yticklabels=labels, \n",
    "        cmap='RdYlBu',\n",
    "        cbar_kws={'label': 'Count'},\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\", fontsize=12)\n",
    "    plt.ylabel(\"True\", fontsize=12)\n",
    "    plt.title(f\"Confusion Matrix - {split_name}\", fontsize=14, pad=10)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return y_pred, report\n",
    "\n",
    "# Evaluate on all sets\n",
    "y_train_pred, train_report = evaluate_model(X_train_feats, y_train, \"Training Set\", crf.classes_)\n",
    "y_val_pred, val_report = evaluate_model(X_val_feats, y_val, \"Validation Set\", crf.classes_)\n",
    "y_test_pred, test_report = evaluate_model(X_test_feats, y_test, \"Test Set\", crf.classes_)\n",
    "\n",
    "# Step 7: Transition Weights Visualization\n",
    "def plot_transition_weights(crf, top_n=20):\n",
    "    transitions = crf.transition_features_\n",
    "    sorted_trans = sorted(transitions.items(), key=lambda x: abs(x[1]), reverse=True)[:top_n]\n",
    "    from_labels, to_labels = zip(*[t[0] for t in sorted_trans])\n",
    "    weights = [t[1] for t in sorted_trans]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=[f\"{f} → {t}\" for f, t in zip(from_labels, to_labels)], \n",
    "        y=weights, \n",
    "        hue=[f\"{f} → {t}\" for f, t in zip(from_labels, to_labels)], \n",
    "        palette=\"magma\", \n",
    "        legend=False\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(\"Top Transition Weights in CRF\", fontsize=14)\n",
    "    plt.xlabel(\"Transitions\", fontsize=12)\n",
    "    plt.ylabel(\"Weight\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_transition_weights(crf)\n",
    "\n",
    "# Step 8: F1-Score Comparison Plot\n",
    "def plot_f1_comparison(train_report, val_report, test_report, title):\n",
    "    classes = [c for c in val_report.keys() if c not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "    train_f1 = [train_report[c]['f1-score'] for c in classes]\n",
    "    val_f1 = [val_report[c]['f1-score'] for c in classes]\n",
    "    test_f1 = [test_report[c]['f1-score'] for c in classes]\n",
    "    \n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(x - width, train_f1, width, label='Training', color='#FF6F61', edgecolor='black')\n",
    "    plt.bar(x, val_f1, width, label='Validation', color='#6B5B95', edgecolor='black')\n",
    "    plt.bar(x + width, test_f1, width, label='Test', color='#88B04B', edgecolor='black')\n",
    "    plt.xlabel('Classes', fontsize=12)\n",
    "    plt.ylabel('F1-Score', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xticks(x, classes, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_f1_comparison(train_report, val_report, test_report, \"F1-Score Comparison: Training vs Validation vs Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019f83dd-ad6f-413b-a262-26a17f346770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdd129-a215-4615-ab01-f3f052806895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
